<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ONNX.js CDN Test Interface</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
        #debug { white-space: pre-wrap; border: 1px solid #ccc; padding: 10px; margin-top: 20px; }
    </style>
</head>
<body>
    <h1>ONNX.js CDN Test Interface</h1>
    <button onclick="loadAndTestModel()">Load and Test Model</button>
    <div id="debug"></div>

    <script>
        const debugElement = document.getElementById('debug');

        function log(message) {
            debugElement.textContent += message + '\n';
            console.log(message);
        }

        async function loadAndTestModel() {
            debugElement.textContent = '';  // Clear previous debug output
            log('Starting model load and test...');

            try {
                const session = new onnx.InferenceSession();
                log('InferenceSession created');

                // Load a simple pre-trained ONNX model (Sigmoid)
                const modelUrl = 'https://github.com/microsoft/onnxjs/raw/master/test/data/node/test_sigmoid/model.onnx';
                await session.loadModel(modelUrl);
                log('Simple model loaded successfully from CDN');

                // Test inference
                const inputTensor = new onnx.Tensor(new Float32Array([1, 2, 3, 4]), 'float32', [2, 2]);
                const outputMap = await session.run([inputTensor]);
                const outputTensor = outputMap.values().next().value;
                log('Inference result: ' + outputTensor.data);

                log('Now trying to load your model...');
                const yourSession = new onnx.InferenceSession();
                await yourSession.loadModel('model.onnx');
                log('Your model loaded successfully');

                // Try to run inference on your model
                const yourInputTensor = new onnx.Tensor(new Float32Array([1, 2, 3, 4]), 'float32', [1, 4]);
                const yourOutputMap = await yourSession.run([yourInputTensor]);
                const yourOutputTensor = yourOutputMap.values().next().value;
                log('Your model inference result: ' + yourOutputTensor.data);

            } catch (error) {
                log('Error: ' + error.message);
                console.error(error);
            }
        }
    </script>
</body>
</html>